# Logstash VPC Flow Logs Optimization configuration

# Read VPC Flow Logs from S3 bucket and define the sourcetype
input {
  s3 {
    id => "s3_vpcflowlogs_optimized"
    add_field => {"sourcetype" => "vpc_optimized"}
    bucket => "awsianp-us-east-1-vpc-flow-logs-test"
    prefix => "AWSLogs/49"
    region => "us-east-1"
    temporary_directory => "/data/logstash/tmp/opt49/"
    sincedb_path => "/var/lib/logstash/plugins/inputs/s3/awsianpopt49"
  }
}

filter {
  if [sourcetype] == "vpc_optimized" {

    # Discard messages without data TODO: We should probably send these as is without processing instead
    if "SKIPDATA" in [message] or "NODATA" in [message] {
      drop {
        id => "s3_vpcflowlogs_drop_optimized"
      }
    }

    # Parse each message based on custom VPC Flow Log format
    csv {
      id => "s3_vpcflowlogs_csv_optimized"
      source => "message"
      separator => " "
      autogenerate_column_names => false
      skip_header => true
      columns => [ "version", "account-id", "interface-id", "srcaddr", "dstaddr", "srcport", "dstport", "protocol", "packets", "bytes", "start", "end", "action", "log-status", "vpc-id", "subnet-id", "instance-id", "pkt-srcaddr", "pkt-dstaddr", "tcp-flags", "type"
      ]
    }

    # Hash each message identifying unique flows
    fingerprint {
      id => "s3_vpcflowlogs_fingerprint_optimized"
      concatenate_sources => true
      key => "flows"
      method => "SHA1"
      source => [
              "vpc-id", "pkt-srcaddr", "pkt-dstaddr", "srcport", "dstport", "protocol", "action"
      ]
    }

    # Transform significant fields for processing during aggregate routines
    if [start] {
      mutate {
        id => "s3_vpcflowlogs_mutate_start_optimized"
        convert => {
          "start" => "integer"
        }
      }
      date {
        id => "s3_vpcflowlogs_date_start_optimized"
        match => ["start", "UNIX"]
        target => "@timestamp"
      }
    }
    if [packets] {
      mutate {
        id => "s3_vpcflowlogs_mutate_packets_optimized"
        convert => {
          "packets" => "integer"
        }
      }
    }
    if [bytes] {
      mutate {
        id => "s3_vpcflowlogs_mutate_bytes_optimized"
        convert => {
          "bytes" => "integer"
        }
      }
    }
    if [end] {
      mutate {
        id => "s3_vpcflowlogs_mutate_end_optimized"
        convert => {
          "end" => "integer"
        }
      }
    }

    aggregate {
      id => "s3_vpcflowlogs_aggregate"
      task_id => "%{fingerprint}"
      aggregate_maps_path => "/data/logstash/maps/awsianpopt49"

      # Flush any pending inactive flows before full timeout
#      inactivity_timeout => 600

      # Do not hold any flows beyond timeout
      timeout => 900
      push_map_as_event_on_timeout => true

      timeout_task_id_field => "fingerprint"

      # All timeout events are based on the original message timestamp - Allows accurate processing of old flows
      timeout_timestamp_field => "@timestamp"
      timeout_tags => ["aggregate_timeout"]

      # Build map and initialize with static and aggregate fields to be used as new message upon flow expiration
      # TODO: If events are not received in order we could endup with the wrong initial timestamp - Add conditional check
      # TODO: Should initialize these fields as there is no guarantee we won't have nulls here
      # Drop processing of original event as only the aggregated event will be sent
      code => "
				map['flows'] ||=0; map['flows'] += 1;
				map['firsttimestamp'] ||= event.get('@timestamp')
				map['sum_packets'] ||=0; map['sum_packets'] += event.get('packets')
				map['sum_bytes'] ||=0; map['sum_bytes'] += event.get('bytes')
				map['first_start'] ||= event.get('start')
				map['first_start'] = event.get('start') if map['first_start'] > event.get('start')
				map['last_end'] ||= event.get('end')
				map['last_end'] = event.get('end') if map['last_end'] < event.get('end')
				map['version'] ||= event.get('version')
				map['account-id'] ||= event.get('account-id')
				map['interface-id'] ||= event.get('interface-id')
				map['srcaddr'] ||= event.get('srcaddr')
				map['dstaddr'] ||= event.get('dstaddr')
				map['srcport'] ||= event.get('srcport')
				map['dstport'] ||= event.get('dstport')
				map['protocol'] ||= event.get('protocol')
				map['action'] ||= event.get('action')
				map['log-status'] ||= event.get('log-status')
				map['vpc-id'] ||= event.get('vpc-id')
				map['subnet-id'] ||= event.get('subnet-id')
				map['instance-id'] ||= event.get('instance-id')
				map['pkt-srcaddr'] ||= event.get('pkt-srcaddr')
				map['pkt-dstaddr'] ||= event.get('pkt-dstaddr')
				map['tcp-flags'] ||= event.get('tcp-flags')
				map['type'] ||= event.get('type')
				map['sourcetype'] ||= event.get('sourcetype')
				event.set('state', 'in-process')
#				event.cancel()
			"

      # Update final event with the aggregated data and remove the temporary fields
      timeout_code => "
				event.set('packets', event.get('sum_packets'))
				event.set('bytes', event.get('sum_bytes'))
				event.set('start', event.get('first_start'))
				event.set('end', event.get('last_end'))
				event.set('@timestamp', event.get('firsttimestamp'))
				event.remove('sum_packets')
				event.remove('sum_bytes')
				event.remove('first_start')
				event.remove('last_end')
				event.remove('firsttimestamp')
				event.remove('state')
			"
      map_action => "create_or_update"
    }
  }
}


output {
  if [sourcetype] == "vpc_optimized" and "aggregate_timeout" in [tags] {
#    elasticsearch {
#      hosts => ["localhost:9200"]
#      id => "s3_vpcflowlogs_optimized_es"
#      index => "vpcflow-logs-01-%{+YYYY.MM.dd}"
#    }
    file {
      id => "s3_vpcflowlogs_optimized_file"
      path=> "/data/logstash/output/file/%{vpc-id}/%{+YYYY}/%{+MM}/%{+dd}/%{+YYYY-MM-dd-HH}-vpc-flow-opt.csv"
      codec => line { format => "%{version} %{account-id} %{interface-id} %{pkt-srcaddr} %{pkt-dstaddr} %{srcport} %{dstport} %{protocol} %{packets} %{bytes} %{start} %{end} %{action} %{log-status} %{vpc-id} %{subnet-id} %{instance-id} %{tcp-flags} %{type} %{flows}"}
    }
  }
  if [sourcetype] == "vpc_optimized" and ([tags] and ("aggregate_timeout" not in [tags] and [state] == "in-process")) {
    stdout {
      id => "s3_vpcflowlogs_optimized_stdout"
      codec => rubydebug
    }
  }
}